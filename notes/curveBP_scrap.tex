\documentclass[12pt]{article}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage[nameinlink,capitalize]{cleveref}
\usepackage{cleveref}
\usepackage[margin=1in]{geometry}
\usepackage{lineno}\renewcommand\thelinenumber{\color{gray}\arabic{linenumber}}
\usepackage{pdflscape}
\usepackage{xspace}
\usepackage{array}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\comment}{\showcomment}
\newcommand{\showcomment}[3]{\textcolor{#1}{\textbf{[#2: }\textsl{#3}\textbf{]}}}
\newcommand{\nocomment}[3]{}

\newcommand{\ali}[1]{\comment{magenta}{Ali}{#1}}
\newcommand{\bmb}[1]{\comment{red}{BMB}{#1}}
\newcommand{\todo}[1]{\comment{red}{TODO}{#1}}

\theoremstyle{definition} % amsthm only
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
 
\bibliographystyle{apalike}

\title{Centrality: A Curve-Based Statistical Discription of an Ensemble}
\author{Ali Gharouni, Ben Bolker}
\begin{document}
\maketitle
\linenumbers

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%% However, the reason/potential advantage of aparting from the standard functional boxplot framework is unclear \ali{to be toned down a bit?!}.  

%% presented the following useful curve-based alternative methods to the fixed-time statistics of epidemic curve ensembles; (i) all-or-nothing ranking method (presented in their Fig.2b,c), (ii) weighted ranking (presented in their Fig.2d) and (iii) according to some feature of interest, e.g. the projected peak value (presented in their Fig.2e). Note that (i) and (ii) are sampling-based approaches and (iii) is a featur-based ranking approach. They cautioned that standard pointwise (fixed-time) averages may not be appropriate to summerize ensembles of epidemic curves. Particularly, they discussed that miscapturing key features of an epidemic such as the peak numbers of infections, the time of the peak, etc. may result in obscuring the forcast process. While Juul et al.'s implimented their methodology in sampling-based functional ranking and establishing the most ``central set'' of an ensemble, there is a deep existing literature in functional depth, functional boxplot, and centrality metrics for high dimension data \citep{fraiman2001trimmed, lopez2007depth, lopez2009concept, sun2011functional,sun2012exact}. It appears that \cite{juul2021fixed} were aware of the functional boxplot concept, since they cited the work by \cite{sun2011functional}. However, the reason/potential advantage of aparting from the standard functional boxplot framework is unclear \ali{to be toned down a bit?!}.  


%The idea of the functional boxplot goes back farther to the notion of depth. It was first introduced for multivariate data in an attempt to generalize the ideas of order statistics, ranks and medians into higher dimensions (see for e.g., \cite{mahalanobis1936generalized,tukey1975mathematics}). The notion of depth was extended for functional data by \citep{lopez2009concept}. They introduced the concept of band depth (BD) for ranking a sample of functional data from the center outward. This ordering enables us to define the functional quantiles, centrality or outlyingness of an observation. Further, the classical boxplot for univariate data was extended to the functional boxplots and adjusted functional boxplots as a visualization tool \citep{sun2011functional,sun2012adjusted}.

%We note that \cite{juul2021fixed}'s sampling-based approach is a special case of functional depth in which the robustness and optimized computational cost of the functional band depth is known \citep{sun2012exact}. The computational cost is determined by the sample size of the ensemble $n$ and the number of curves $j$ defining a band/envelope ($2\leq j \leq J$). \cite{sun2012exact} discussed the robustness and sensibility of small values of $J$, and in particular $J=2$ is suggested for large datasets. 
%It is notable that \cite{juul2021fixed}'s sampling-based approach is a special case of functional depth, where $n=500$ and $J=50$ (in their paper: $N_{\rm{curves}}=50$) and random samples were uniformly drawn $N_{\rm{samples}}=100$ times. Likewise the functional depth method, the centrality scores of all curves in the ensemble were updated based on whether the entire curve is contained in the sample-specific envelope. The most central curves are the ones with their scores above the 50\% percentile of scores. Here, we use the functional boxplot on \cite{juul2021fixed}'s dataset, thus ${500}\choose{2}$ samples determine the centrality scores.


\bibliography{../AliMac}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix}


Goal: 
 How to compare Juul's result to ours, quantitative/visual? the point is that in addition to the option that Juul et al gave, here we present other options and you should think about which ones have the properties that you want.
 
What to be presented? 
- maybe 4 panels, here is what you get by various methods. Each panel have several quantile overlaid (50,80,90) 1 Juul's work, 1 Mahalanobis on the probes,  

Notes:
1) The central region (not the curve) is to be focused on. (spend a paragraph) 

2) What did Juul do? Algorithm? 
Out of the whole curves, referred as the whole ensemble, pick a random sensible, consider the pointwise min and max which determines the envelope. $E_{sample}$ is the envelope of the sample, assign a score to each curve in the ensemble, all scores starts with 0 if the curve is not in the envelope, they add $s(c_i)$ but we think the default is constant 1, then you can weight it in some way. 

Juul's concept of centrality goes back to a 2009 paper \cite{lopez2009concept}, recommending of sampling 3 curves at a time (check?). 
- Where Juul's method comes from?
- How depth handles phase variation?
- Search for functional band depth in R, check fbplot in fda package \url{https://www.rdocumentation.org/packages/fda/versions/5.1.9/topics/fbplot} how to specify the size of subensemble.
- check also \cite{sun2012exact}

- This implies that the centrality score cannot be lower than the resampled proportion. 
That is, if the size of a subensemble is $p\%$ of the whole ensemble, any curve in the subensemble will have score of 1 at least $p\%$ of the time but the centrality score cannot be less that $p$. Specifically, each curve in the whole ensemble has a $p\%$ chance to be picked in the subensemble, thus be in the envelope, so the centrality score cannot be less than the size of the subensemble.     
- Is there a upper bound? We are not sure. Thinking about a straight line with other curves are curved up or down etc.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Literature Review}
\citep{juul2021fixed}

\cite{probert2016decision} looking at different metrics (probes) of an epidemic.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{emails}

{\bf Ben,  
date: Feb 9, 2021}

There are several somewhat orthogonal questions, all geared to the general question of "what is a sensible way to define a confidence interval or 'typical' set of curves out of an ensemble?

 1. Do we apply measures of centrality directly to the curves or to
probes derived from the curves?
 2.  What measure of centrality/typical-ness do we apply?
 3.  If it depends on distance, what distance do we use?
 4. Do we define centrality by difference from a central point
(centroid/Fr√©chet mean) or by average closeness to the rest of the
set? Does the answer differ depending on step 3 (choice of distance)?

(And finally: how do Juul et al's methods fit into this?)

Another email the same day:
you might be able to show that the conjecture does *not* hold for some fairly simple distance metric (e.g. log(Euclidean) or some similar kind of geometric-mean distance), in which case it would also settle most of the question.




\end{document}
